{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/train.csv')\n",
    "test = pd.read_csv('../Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toKaggle(test,pred):\n",
    "    \n",
    "\n",
    "    resp = test.copy()\n",
    "    resp['SalePrice'] = pred\n",
    "    resp_f = resp.loc[:,['Id','SalePrice']]\n",
    "    \n",
    "    return resp_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(train,test):\n",
    "    # Mean imputation\n",
    "    train = train.fillna(train.mean())\n",
    "    # Other label for categorical\n",
    "    train = train.fillna('Other')\n",
    "\n",
    "    # Lets get X and Y\n",
    "    X_train = train.drop('SalePrice', axis =1)\n",
    "    y_train = train.SalePrice\n",
    "\n",
    "    test_X = test.drop('Id',axis = 1)\n",
    "    # Mean inputation for continuos.\n",
    "    test_X = test_X.fillna(test_X.mean())\n",
    "    # Other label for categorical\n",
    "    test_X = test_X.fillna('Other')\n",
    "    \n",
    "    return X_train,y_train,test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train,test_X = pre_process(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alpha', 'criterion', 'init', 'learning_rate', 'loss', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_iter_no_change', 'presort', 'random_state', 'subsample', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingRegressor().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBoost_w_GS(X_train,y_train):\n",
    "    \n",
    "    categorical = X_train.dtypes == object\n",
    "\n",
    "    # Lets do the column transformer with Standar Scaler and One-Hot Encoder.\n",
    "    preprocess = make_column_transformer(\n",
    "        (RobustScaler(), ~categorical),\n",
    "        (OneHotEncoder(handle_unknown = 'ignore'), categorical))\n",
    "\n",
    "    model_XGB = make_pipeline(preprocess, GradientBoostingRegressor(alpha = 0.85,n_estimators = 1000,max_depth = 3, max_features = 70))\n",
    "\n",
    "\n",
    "    # Linear Regression.\n",
    "\n",
    "    param_grid_XGB = {#'gradientboostingregressor__max_depth': range(1,10,1), #  3.\n",
    "                      #'gradientboostingregressor__max_features':range(1,80,4)#, # 70\n",
    "                      #'gradientboostingregressor__alpha':np.logspace(-2, -0.001, num=30) # 0.85\n",
    "                      }\n",
    "\n",
    "    grid_XGB = GridSearchCV(model_XGB,param_grid_XGB, cv=20)\n",
    "    grid_XGB.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\t\\t\\t XGBoost: \\n')\n",
    "    print(grid_XGB.best_params_, '\\t score: ',grid_XGB.score(X_train, y_train),'\\n')\n",
    "    \n",
    "    return grid_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t XGBoost: \n",
      "\n",
      "{} \t score:  0.9970868178237801 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_GB = GBoost_w_GS(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb = grid_GB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tune max_features\n",
    "* Tune column subsampling, row subsampling\n",
    "* Typically strong pruning via max_depth\n",
    "* Regularization\n",
    "* Tune learning rate and do early stopping?\n",
    "\n",
    "#### Strategies for tuning: Ranges\n",
    "* Max depth: He goes 1,2,3. People sometimes go to 20.\n",
    "* Learning rate: 0.1,0.01,0.001,etc\n",
    "* Regularization: Logarithmic scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBM(X_train,y_train):\n",
    "\n",
    "    categorical = X_train.dtypes == object\n",
    "\n",
    "    # Lets do the column transformer with Standar Scaler and One-Hot Encoder.\n",
    "\n",
    "    preprocess = make_column_transformer(\n",
    "        (RobustScaler(), ~categorical),\n",
    "        (OneHotEncoder(handle_unknown = 'ignore'), categorical))\n",
    "\n",
    "    model_LGB = make_pipeline(preprocess, LGBMClassifier(objective='multiclass', random_state=5))\n",
    "\n",
    "\n",
    "    # Linear Regression.\n",
    "\n",
    "    param_grid_LGB = {#'gradientboostingregressor__max_depth': range(1,10,1), # Gradient boosting showed 3.\n",
    "                      #'gradientboostingregressor__max_features':range(1,80,4)#, # 70 was normally how much it used\n",
    "                      #'gradientboostingregressor__alpha':np.logspace(-2, -0.001, num=30) # 0.85\n",
    "                      }\n",
    "\n",
    "    grid_LGB = GridSearchCV(model_LGB,param_grid_LGB, cv=20)\n",
    "\n",
    "    grid_LGB.fit(X_train, y_train)\n",
    "\n",
    "    return grid_LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_LGB = LGBM(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgbm = grid_LGB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 0.5*y_pred_gb + 0.5*y_pred_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = toKaggle(test,pred) # We have grid_R, grid_L, grid_EN, grid_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.to_csv('resp_L.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = toKaggle(grid_GB,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.to_csv('resp_GB.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
